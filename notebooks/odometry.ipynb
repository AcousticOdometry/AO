{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRorKX0aJrmb"
   },
   "source": [
    "This notebook is part of Andreu's (esdandreu@gmail.com) Master Thesis work at\n",
    "Keio University.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AcousticOdometry/AO/blob/main/notebooks/odometry.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2Q8wXiqTp00"
   },
   "source": [
    "# Setup\n",
    "\n",
    "This section will take care of installing the necessary packages as well as\n",
    "configuring some environment variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab\n",
    "\n",
    "Assess wether the notebook is being executed in [Google\n",
    "Colab](https://colab.research.google.com/) and if so, set up the software\n",
    "needed in Colab runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mxqlh9G3n3sF"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "try:\n",
    "    from google import colab\n",
    "    COLAB_RUNTIME = True\n",
    "    %pip install torchinfo\n",
    "    colab.drive.mount('/content/drive')\n",
    "except ImportError:\n",
    "    COLAB_RUNTIME = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JP7gX_G3UrV_"
   },
   "source": [
    "## Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hFV56apJJicf"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-vDIeFnUup2"
   },
   "source": [
    "### AO\n",
    "\n",
    "Setup Acoustic Odometry python package. If this notebook is being executed in\n",
    "[Colab](#colab), the package will be installed from Github. Because of this, a\n",
    "Github [personal access\n",
    "token](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token)\n",
    "is needed for the installation.\n",
    "\n",
    "If the notebook is not running on Colab and the package is not already\n",
    "installed, installation instructions will be prompted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XZPO4X8roB8k",
    "outputId": "c1fcf943-8933-4af5-89e6-5672e073bd74"
   },
   "outputs": [],
   "source": [
    "if COLAB_RUNTIME:\n",
    "    import subprocess\n",
    "    import requests\n",
    "    import sys\n",
    "    import os\n",
    "    #@markdown Use a [GitHub Personal Access Token](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token)\n",
    "    GITHUB_TOKEN = ''  #@param {type:\"string\"}\n",
    "    auth = requests.auth.HTTPBasicAuth('', GITHUB_TOKEN)\n",
    "    response = requests.get(\n",
    "        \"https://api.github.com/repos/AcousticOdometry/AO/releases/latest\",\n",
    "        auth=auth\n",
    "        )\n",
    "    try:\n",
    "        response.raise_for_status()\n",
    "    except requests.HTTPError as e:\n",
    "        raise RuntimeError(\n",
    "            'Check GITHUB_TOKEN is a Personal Access Token with repo access'\n",
    "            )\n",
    "    headers = {'Accept': 'application/octet-stream'}\n",
    "    for asset in response.json()['assets']:\n",
    "        r = requests.get(\n",
    "            asset['url'], auth=auth, allow_redirects=True, headers=headers\n",
    "            )\n",
    "        r.raise_for_status()\n",
    "        wheel_name = asset['name']\n",
    "        with open(wheel_name, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "        try:\n",
    "            result = subprocess.check_output([\n",
    "                sys.executable, '-m', 'pip', 'install', wheel_name\n",
    "                ])\n",
    "            print(f'Installed {wheel_name}')\n",
    "            break\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            pass\n",
    "        finally:\n",
    "            os.remove(wheel_name)\n",
    "    import ao\n",
    "else:\n",
    "    try:\n",
    "        import ao\n",
    "    except ImportError:\n",
    "        raise ImportError(\n",
    "            \"Acoustic Odometry python extension is not installed. Check \"\n",
    "            r\"https://github.com/AcousticOdometry/AO#readme\"\n",
    "            \" for detailed instructions.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Odometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COLAB_RUNTIME:\n",
    "    # @markdown Check where is the experiment folder situated in your drive folder.\n",
    "    # @markdown Remember that if you have been shared the folder, you can\n",
    "    # @markdown [add a shortcut to your drive](https://support.google.com/drive/answer/9700156?hl=en&co=GENIE.Platform%3DDesktop)\n",
    "    # @markdown in order to make it available in google colab.\n",
    "    experiment = \"/content/drive/MyDrive/VAO_WheelTestBed-Experiment-1\"  #@param {type:\"string\"}\n",
    "    EXPERIMENT_FOLDER = Path(experiment)\n",
    "    if not EXPERIMENT_FOLDER.is_dir():\n",
    "        raise RuntimeError(f'Invalid experiment folder {EXPERIMENT_FOLDER}')\n",
    "else:\n",
    "    EXPERIMENT_FOLDER = ao.dataset.utils.get_folder(\n",
    "        env='WHEELTESTBED_EXPERIMENT2'\n",
    "        )\n",
    "VALIDATION_FOLDER = EXPERIMENT_FOLDER / 'validation-recordings'\n",
    "MODELS_FOLDER = Path().resolve().parent / 'models'\n",
    "print(MODELS_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate recording ground truth\n",
    "\n",
    "The recording is composed by audio files from several devices that span for the\n",
    "whole experiment and several wheel test bed control files that span for certain\n",
    "parts of the experiment. In this section we combine the control files in order\n",
    "to generate a unique ground truth for the whole recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording = VALIDATION_FOLDER / 'date_2022-05-20;time_16-59-56'\n",
    "\n",
    "def get_recording_bounds(recording: Path) -> Tuple[float, float]:\n",
    "    start_timestamp = np.Inf\n",
    "    end_timestamp = 0\n",
    "    for wav_file in recording.glob('microphone*.wav'):\n",
    "        config = ao.io.yaml_load(wav_file.with_suffix('.yaml'))\n",
    "        start_timestamp = min(start_timestamp, config['start_timestamp'])\n",
    "        end_timestamp = max(end_timestamp, config['end_timestamp'])\n",
    "    return start_timestamp, end_timestamp\n",
    "\n",
    "get_recording_bounds(recording=recording)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recording_ground_truth(recording: Path) -> pd.DataFrame:\n",
    "    start_timestamp, end_timestamp = get_recording_bounds(recording)\n",
    "    gt = pd.DataFrame(columns=['X', 'Vx', 'Vw', 'slip', 'wheel_rotation'])\n",
    "    gt.loc[start_timestamp] = np.zeros(len(gt.columns))\n",
    "    for control_file in sorted(recording.glob('*.csv')):\n",
    "        if control_file.stem == 'ground_truth':\n",
    "            continue\n",
    "        df = pd.read_csv(\n",
    "            control_file,\n",
    "            index_col='timestamp',\n",
    "            usecols=['timestamp'] + list(gt.columns),\n",
    "            parse_dates=True,\n",
    "            )\n",
    "        # Accumulable columns\n",
    "        for col in ['X', 'wheel_rotation']:\n",
    "            df[col] = df[col] + gt[col].iloc[-1]\n",
    "        # Wheel stops at the end of the movement, assume stopped after 100 ms\n",
    "        df.loc[df.tail(1).index.item() + 0.1] = df.iloc[-1].copy()\n",
    "        # Update the first and last row with null velocities\n",
    "        for col in ['Vx', 'Vw', 'slip']:\n",
    "            col_index = gt.columns.get_loc(col)\n",
    "            df.iloc[0, col_index] = 0\n",
    "            df.iloc[-1, col_index] = 0\n",
    "        # After each control the wheel stops\n",
    "        gt = pd.concat([gt, df])\n",
    "    gt.loc[end_timestamp] = gt.iloc[-1].copy()\n",
    "    for col in ['Vx', 'Vw', 'slip']:\n",
    "        col_index = gt.columns.get_loc(col)\n",
    "        gt.iloc[-1, col_index] = 0\n",
    "    gt.loc[:, 'X'] /= 1000 # [mm] -> [m]\n",
    "    return gt\n",
    "\n",
    "ground_truth = generate_recording_ground_truth(recording=recording)\n",
    "ground_truth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recording_ground_truth(\n",
    "        recording: Path, TUM_format: bool = False\n",
    "    ) -> pd.DataFrame:\n",
    "    ground_truth_path = recording / 'ground_truth.csv'\n",
    "    if ground_truth_path.exists():\n",
    "        gt = pd.read_csv(ground_truth_path, index_col='timestamp')\n",
    "    else:\n",
    "        gt = generate_recording_ground_truth(recording)\n",
    "        gt.to_csv(ground_truth_path, index_label='timestamp')\n",
    "    if TUM_format:\n",
    "        gt['tx'] = gt['X'].diff()\n",
    "        gt.drop(\n",
    "            columns=['X', 'Vx', 'Vw', 'slip', 'wheel_rotation'], inplace=True\n",
    "            )\n",
    "        gt[['ty', 'tz', 'qw', 'qx', 'qy', 'qz']] = 0\n",
    "        gt.iloc[0, :] = 0\n",
    "    return gt\n",
    "\n",
    "\n",
    "ground_truth = get_recording_ground_truth(recording=recording)\n",
    "ax = ground_truth.plot(y=['Vx'], ylabel='Speed [m/s]', color='orange')\n",
    "ground_truth.plot(ax=ax, y=['X'], secondary_y=True).set_ylabel('Position [m]')\n",
    "get_recording_ground_truth(recording=recording, TUM_format=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition\n",
    "\n",
    "To be refactored into AO package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "\n",
    "class AcousticOdometryModel(nn.Module):\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "# Reset the subclasses to allow changes without restarting the kernel\n",
    "for subclass in AcousticOdometryModel.__subclasses__():\n",
    "    del subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNet(AcousticOdometryModel):\n",
    "    def __init__(self, classes: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(210816, 512)\n",
    "        self.fc2 = nn.Linear(512, classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        #x = x.view(x.size(0), -1)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.log_softmax(x,dim=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AcousticOdometryModel.__subclasses__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply model to recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acoustic_odometry(wav_file: Path, model_path: Path) -> pd.DataFrame:\n",
    "    model = torch.load(model_path)\n",
    "    # TODO This should be loaded from the model configuration\n",
    "    segment_duration = 1200  # [milliseconds]\n",
    "    segment_overlap = 1000  # [milliseconds]\n",
    "    frame_features = 256\n",
    "    frame_duration = 10  # [milliseconds])\n",
    "    extractor = ao.extractor.GammatoneFilterbank\n",
    "    extractor_kwargs = {}\n",
    "    compression = math.log10\n",
    "\n",
    "    wav_data, sample_rate = ao.io.wave_read(wav_file)\n",
    "    config = ao.io.yaml_load(wav_file.with_suffix('.yaml'))\n",
    "    start = config['start_timestamp']\n",
    "    step = (segment_duration - segment_overlap) / 1000\n",
    "    frame_samples = int(frame_duration * sample_rate / 1000)\n",
    "    if compression:\n",
    "        compression = np.vectorize(compression)\n",
    "    extract = extractor(\n",
    "        num_samples=frame_samples,\n",
    "        num_features=frame_features,\n",
    "        sample_rate=sample_rate,\n",
    "        **extractor_kwargs\n",
    "        )\n",
    "    # TODO this should be done in a streaming fashion using a C++ interface\n",
    "    segments = ao.dataset.audio.segment(\n",
    "        wav_data, sample_rate, segment_duration, segment_overlap\n",
    "        )\n",
    "    timestamps = np.linspace(\n",
    "        start, start + len(segments) * step, len(segments)\n",
    "        )\n",
    "    Vx = np.zeros(len(segments))\n",
    "    for i, segment in enumerate(segments):\n",
    "        # ! This is very inefficient as we are actually recomputing the\n",
    "        # ! features from the overlapped segment which is 90% of its size\n",
    "        features = torch.from_numpy(\n",
    "            ao.dataset.audio.features(\n",
    "                segment,\n",
    "                frame_samples,\n",
    "                extract=extract,\n",
    "                compression=compression\n",
    "                )[np.newaxis, np.newaxis, :, :]  # [batch, channel, w, h]\n",
    "            ).float().to(DEVICE)\n",
    "        Vx[i] = model(features).argmax(1).sum().item()\n",
    "        # Free memory\n",
    "        del features\n",
    "\n",
    "    # Compute X translations and cumulative X position\n",
    "    Vx = pd.Series(Vx, index=timestamps) / 100  # [cm/s] -> [m/s]\n",
    "    odom = pd.concat([Vx, Vx.index.to_series().diff() * Vx], axis=1)\n",
    "    odom.columns = ['Vx', 'tx']\n",
    "    odom.iloc[0, :] = 0\n",
    "    odom['X'] = odom['tx'].cumsum()\n",
    "    return odom\n",
    "\n",
    "\n",
    "odom = acoustic_odometry(\n",
    "    wav_file=recording / f'microphone0.wav',\n",
    "    model_path=MODELS_FOLDER /\n",
    "    'name_numpy-arrays;date_2022-05-23;time_13-39-14.pt'\n",
    "    )\n",
    "odom.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acoustic_odometry(odom: pd.DataFrame, ground_truth: pd.DataFrame):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    axs[0].set_title('Acoustic Odometry')\n",
    "    ax = odom.plot(ax=axs[0], y=['Vx'], ylabel='Speed [m/s]', color='orange')\n",
    "    odom.plot(ax=ax, y=['X'], secondary_y=True).set_ylabel('Position [m]')\n",
    "    axs[1].set_title('Speed validation')\n",
    "    ax = ground_truth.plot(\n",
    "        ax=axs[1],\n",
    "        y=['Vx'],\n",
    "        ylabel='Speed [m/s]',\n",
    "        color='orange',\n",
    "        label=['Ground truth']\n",
    "        )\n",
    "    odom.plot(ax=ax, y=['Vx'], label=['Acoustic Odometry'])\n",
    "    axs[2].set_title('Position validation')\n",
    "    ax = ground_truth.plot(\n",
    "        ax=axs[2],\n",
    "        y=['X'],\n",
    "        ylabel='Position [m]',\n",
    "        color='orange',\n",
    "        label=['Ground truth']\n",
    "        )\n",
    "    odom.plot(ax=ax, y=['X'], label=['Acoustic Odometry'])\n",
    "    fig.tight_layout()\n",
    "    return fig, axs\n",
    "\n",
    "_ = plot_acoustic_odometry(odom, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_acoustic_odometry(recording: Path, microphone: int):\n",
    "    ground_truth = get_recording_ground_truth(recording=recording)\n",
    "    wav_file = recording / f'microphone{microphone}.wav'\n",
    "    odom = acoustic_odometry(\n",
    "        wav_file=wav_file,\n",
    "        model_path=MODELS_FOLDER /\n",
    "        'name_numpy-arrays;date_2022-05-23;time_13-39-14.pt'\n",
    "        )\n",
    "    fig, _ = plot_acoustic_odometry(odom, ground_truth)\n",
    "    # Load config and set title\n",
    "    config = ao.io.yaml_load(wav_file.with_suffix('.yaml'))\n",
    "    fig.suptitle(f\"microphone {microphone}: {config['name']}\")\n",
    "    fig.tight_layout()\n",
    "    \n",
    "validate_acoustic_odometry(recording=recording, microphone=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_acoustic_odometry(recording=recording, microphone=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_acoustic_odometry(recording=recording, microphone=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_acoustic_odometry(recording=recording, microphone=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_acoustic_odometry(recording=recording, microphone=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_acoustic_odometry(recording=recording, microphone=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from evo.tools import file_interface\n",
    "\n",
    "odom_TUM = odom.drop(['Vx', 'X'], axis=1)\n",
    "odom_TUM[['ty', 'tz', 'qw', 'qx', 'qy', 'qz']] = 0\n",
    "print(odom_TUM.head())\n",
    "\n",
    "with tempfile.TemporaryDirectory() as folder:\n",
    "    est_file = Path(folder) / 'est.csv'\n",
    "    odom_TUM.to_csv(est_file, index_label='timestamp', sep=' ', header=False)\n",
    "    traj_est = file_interface.read_tum_trajectory_file(est_file)\n",
    "print(traj_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_traj_ref = get_recording_ground_truth(recording=recording, TUM_format=True)\n",
    "print(_traj_ref.head())\n",
    "with tempfile.TemporaryDirectory() as folder:\n",
    "    ref_file = Path(folder) / 'ref.csv'\n",
    "    _traj_ref.to_csv(ref_file, index_label='timestamp', sep=' ', header=False)\n",
    "    traj_ref = file_interface.read_tum_trajectory_file(ref_file)\n",
    "print(traj_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evo.core import sync\n",
    "\n",
    "max_diff = 0.01\n",
    "\n",
    "traj_ref, traj_est = sync.associate_trajectories(traj_ref, traj_est, max_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evo.core import metrics\n",
    "from evo.tools import plot\n",
    "from pprint import pprint\n",
    "\n",
    "ape_metric = metrics.APE(metrics.PoseRelation.translation_part)\n",
    "ape_metric.process_data((traj_ref, traj_est))\n",
    "\n",
    "ape_stats = ape_metric.get_all_statistics()\n",
    "pprint(ape_stats)\n",
    "seconds_from_start = [t - traj_est.timestamps[0] for t in traj_est.timestamps]\n",
    "fig = plt.figure()\n",
    "plot.error_array(fig.gca(), ape_metric.error, x_array=seconds_from_start,\n",
    "                 statistics={s:v for s,v in ape_stats.items() if s != \"sse\"},\n",
    "                 name=\"APE\", title=\"APE w.r.t. \" + ape_metric.pose_relation.value, xlabel=\"$t$ (s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpe_metric = metrics.RPE(\n",
    "    pose_relation=metrics.PoseRelation.translation_part,\n",
    "    delta=1,\n",
    "    delta_unit=metrics.Unit.frames,\n",
    "    all_pairs=False\n",
    "    )\n",
    "rpe_metric.process_data((traj_ref, traj_est))\n",
    "\n",
    "rpe_stats = rpe_metric.get_all_statistics()\n",
    "pprint(rpe_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seconds_from_start = [\n",
    "    t - traj_est.timestamps[0] for t in traj_est.timestamps[1:]\n",
    "    ]\n",
    "\n",
    "fig = plt.figure()\n",
    "plot.error_array(\n",
    "    fig.gca(),\n",
    "    rpe_metric.error,\n",
    "    x_array=seconds_from_start,\n",
    "    statistics={s: v\n",
    "                for s, v in rpe_stats.items() if s != \"sse\"},\n",
    "    name=\"RPE\",\n",
    "    title=\"RPE w.r.t. \" + rpe_metric.pose_relation.value,\n",
    "    xlabel=\"$t$ (s)\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b8749b76b3be249eef7e116ab90f03825c0416a2284545ab24c0f7f41fa4add3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
