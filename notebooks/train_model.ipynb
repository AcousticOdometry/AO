{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRorKX0aJrmb"
   },
   "source": [
    "This notebook is part of Andreu's (esdandreu@gmail.com) Master Thesis work at\n",
    "Keio University.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AcousticOdometry/AO/blob/main/notebooks/train_model.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2Q8wXiqTp00"
   },
   "source": [
    "# Setup\n",
    "\n",
    "This section will take care of installing the necessary packages as well as\n",
    "configuring some environment variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab\n",
    "\n",
    "Assess wether the notebook is being executed in [Google\n",
    "Colab](https://colab.research.google.com/) and if so, set up the software\n",
    "needed in Colab runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mxqlh9G3n3sF"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "try:\n",
    "    from google import colab\n",
    "    COLAB_RUNTIME = True\n",
    "    colab.drive.mount('/content/drive')\n",
    "except ImportError:\n",
    "    COLAB_RUNTIME = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JP7gX_G3UrV_"
   },
   "source": [
    "## Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hFV56apJJicf"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from typing import Dict\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchinfo import summary\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-vDIeFnUup2"
   },
   "source": [
    "### AO\n",
    "\n",
    "Setup Acoustic Odometry python package. If this notebook is being executed in\n",
    "[Colab](#colab), the package will be installed from Github. Because of this, a\n",
    "Github [personal access\n",
    "token](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token)\n",
    "is needed for the installation.\n",
    "\n",
    "If the notebook is not running on Colab and the package is not already\n",
    "installed, installation instructions will be prompted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XZPO4X8roB8k",
    "outputId": "c1fcf943-8933-4af5-89e6-5672e073bd74"
   },
   "outputs": [],
   "source": [
    "if COLAB_RUNTIME:\n",
    "    import subprocess\n",
    "    import requests\n",
    "    import sys\n",
    "    import os\n",
    "    #@markdown Use a [GitHub Personal Access Token](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token)\n",
    "    GITHUB_TOKEN = ''  #@param {type:\"string\"}\n",
    "    auth = requests.auth.HTTPBasicAuth('', GITHUB_TOKEN)\n",
    "    response = requests.get(\n",
    "        \"https://api.github.com/repos/AcousticOdometry/AO/releases/latest\",\n",
    "        auth=auth\n",
    "        )\n",
    "    try:\n",
    "        response.raise_for_status()\n",
    "    except requests.HTTPError as e:\n",
    "        raise RuntimeError(\n",
    "            'Check GITHUB_TOKEN is a Personal Access Token with repo access'\n",
    "            )\n",
    "    headers = {'Accept': 'application/octet-stream'}\n",
    "    for asset in response.json()['assets']:\n",
    "        r = requests.get(\n",
    "            asset['url'], auth=auth, allow_redirects=True, headers=headers\n",
    "            )\n",
    "        r.raise_for_status()\n",
    "        wheel_name = asset['name']\n",
    "        with open(wheel_name, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "        try:\n",
    "            result = subprocess.check_output([\n",
    "                sys.executable, '-m', 'pip', 'install', wheel_name\n",
    "                ])\n",
    "            print(f'Installed {wheel_name}')\n",
    "            break\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            pass\n",
    "        finally:\n",
    "            os.remove(wheel_name)\n",
    "    import ao\n",
    "else:\n",
    "    try:\n",
    "        import ao\n",
    "    except ImportError:\n",
    "        raise ImportError(\n",
    "            \"Acoustic Odometry python extension is not installed. Check \"\n",
    "            r\"https://github.com/AcousticOdometry/AO#readme\"\n",
    "            \" for detailed instructions.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_folder = Path().parent / 'logs'\n",
    "logging_folder.mkdir(exist_ok=True)\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setLevel(logging.INFO)\n",
    "logging.basicConfig(\n",
    "    format=\"[%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\n",
    "            logging_folder / datetime.now().strftime('%Y%m%d_%H%M%S.log')\n",
    "            ),\n",
    "        stream_handler,\n",
    "        ],\n",
    "    level=logging.DEBUG\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(105408, 300)\n",
    "        self.fc2 = nn.Linear(300, len(dataset.classes))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        #x = x.view(x.size(0), -1)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.log_softmax(x,dim=1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COLAB_RUNTIME:\n",
    "    # @markdown Check where is the experiment folder situated in your drive folder.\n",
    "    # @markdown Remember that if you have been shared the folder, you can\n",
    "    # @markdown [add a shortcut to your drive](https://support.google.com/drive/answer/9700156?hl=en&co=GENIE.Platform%3DDesktop)\n",
    "    # @markdown in order to make it available in google colab.\n",
    "    experiment = \"/content/drive/MyDrive/VAO_WheelTestBed-Experiment-1\"  #@param {type:\"string\"}\n",
    "    EXPERIMENT_FOLDER = Path(experiment)\n",
    "    if not EXPERIMENT_FOLDER.is_dir():\n",
    "        raise RuntimeError(f'Invalid experiment folder {EXPERIMENT_FOLDER}')\n",
    "else:\n",
    "    EXPERIMENT_FOLDER = ao.dataset.utils.get_folder(\n",
    "        env='WHEELTESTBED_EXPERIMENT1'\n",
    "        )\n",
    "DATASETS_FOLDER = EXPERIMENT_FOLDER / 'datasets'\n",
    "MODELS_FOLDER = EXPERIMENT_FOLDER.parent.parent / 'models'\n",
    "MODELS_FOLDER.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(\n",
    "    dataset_folder: Path,\n",
    "    train_split: float = 0.8,\n",
    "    batch_size: int = 15,\n",
    "    validation_dataset_folder: Path = None,\n",
    "    ) -> Dict[str, torch.utils.data.DataLoader]:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'000': 0, '001': 1, '002': 2, '003': 3, '004': 4, '005': 5, '006': 6}\n"
     ]
    }
   ],
   "source": [
    "dataset_folder = DATASETS_FOLDER / 'initial_cnn_samples'\n",
    "dataset = datasets.ImageFolder(\n",
    "    root=dataset_folder,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor()\n",
    "        ])\n",
    "    )\n",
    "print(dataset.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 32158\n",
      "Testing size: 8040\n"
     ]
    }
   ],
   "source": [
    "#split data to test and train\n",
    "#use 80% to train\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, test_size]\n",
    "    )\n",
    "\n",
    "print(\"Training size:\", len(train_dataset))\n",
    "print(\"Testing size:\", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=15,\n",
    "    num_workers=2,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=15,\n",
    "    num_workers=2,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost function used to determine best parameters\n",
    "cost = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# used to create optimal parameters\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "def test(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, Y in dataloader:\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            pred = model(X)\n",
    "\n",
    "            test_loss += cost(pred, Y).item()\n",
    "            correct += (pred.argmax(1) == Y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    logging.info(f'acc: {(100*correct):>0.1f}%, avg loss: {test_loss:>8f}')\n",
    "\n",
    "\n",
    "def train(dataloader, model, loss, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    # TODO use tqdm\n",
    "    for batch_n, (X, Y) in enumerate(dataloader):\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = cost(pred, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_n % 100 == 0:\n",
    "            loss, current = loss.item(), batch_n * len(X)\n",
    "            logging.debug(f'loss: {loss:>7f}  [{current:>5d}/{size:>5d}]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(dataset, model, epochs, validation_dataset=None):\n",
    "    model.train()\n",
    "    # size = len(dataloader.dataset)\n",
    "    # TODO dataset into dataloaders\n",
    "    # TODO epochs\n",
    "    # TODO use tqdm\n",
    "    for batch_n, (X, Y) in enumerate(dataloader):\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = cost(pred, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_n % 100 == 0:\n",
    "            loss, current = loss.item(), batch_n * len(X)\n",
    "            logging.debug(f'loss: {loss:>7f}  [{current:>5d}/{size:>5d}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch 0\n",
      "[INFO] \n",
      "Test Error:\n",
      "acc: 66.2%, avg loss: 0.063582\n",
      "\n",
      "[INFO] Epoch 1\n",
      "[INFO] \n",
      "Test Error:\n",
      "acc: 75.7%, avg loss: 0.049014\n",
      "\n",
      "[INFO] Epoch 2\n",
      "[INFO] \n",
      "Test Error:\n",
      "acc: 81.4%, avg loss: 0.037889\n",
      "\n",
      "[INFO] Epoch 3\n",
      "[INFO] \n",
      "Test Error:\n",
      "acc: 84.1%, avg loss: 0.032734\n",
      "\n",
      "[INFO] Epoch 4\n",
      "[INFO] \n",
      "Test Error:\n",
      "acc: 83.7%, avg loss: 0.031658\n",
      "\n",
      "[INFO] Epoch 5\n",
      "[INFO] \n",
      "Test Error:\n",
      "acc: 86.5%, avg loss: 0.027330\n",
      "\n",
      "[INFO] Epoch 6\n",
      "[INFO] \n",
      "Test Error:\n",
      "acc: 87.2%, avg loss: 0.025854\n",
      "\n",
      "[INFO] Epoch 7\n",
      "[INFO] \n",
      "Test Error:\n",
      "acc: 88.0%, avg loss: 0.024643\n",
      "\n",
      "[INFO] Epoch 8\n",
      "[INFO] \n",
      "Test Error:\n",
      "acc: 88.9%, avg loss: 0.022725\n",
      "\n",
      "[INFO] Epoch 9\n",
      "[INFO] \n",
      "Test Error:\n",
      "acc: 89.3%, avg loss: 0.022265\n",
      "\n",
      "[INFO] Epoch 10\n",
      "[INFO] \n",
      "Test Error:\n",
      "acc: 89.6%, avg loss: 0.021928\n",
      "\n",
      "[INFO] Epoch 11\n",
      "[INFO] \n",
      "Test Error:\n",
      "acc: 89.9%, avg loss: 0.021545\n",
      "\n",
      "[INFO] Epoch 12\n",
      "[INFO] \n",
      "Test Error:\n",
      "acc: 90.4%, avg loss: 0.020569\n",
      "\n",
      "[INFO] Epoch 13\n",
      "[INFO] \n",
      "Test Error:\n",
      "acc: 90.4%, avg loss: 0.020617\n",
      "\n",
      "[INFO] Epoch 14\n",
      "[INFO] \n",
      "Test Error:\n",
      "acc: 90.1%, avg loss: 0.020479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "\n",
    "for t in range(epochs):\n",
    "    logging.info(f'Epoch {t}')\n",
    "    train(train_dataloader, model, cost, optimizer)\n",
    "    test(test_dataloader, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, input_size=(15, 3, 256, 120))\n",
    "torch.save(model, MODELS_FOLDER / 'initial_cnn_samples.pt')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b8749b76b3be249eef7e116ab90f03825c0416a2284545ab24c0f7f41fa4add3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
